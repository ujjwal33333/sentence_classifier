{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "MR.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d5ac1e702a2c49c7aed8a86c4c1f904d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_3fa1c5bc88344682a1206a6071dbd444",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_b2c10e212eab4810a4735048e0e75127",
              "IPY_MODEL_ed838d06c88f473f8fc0abbf72f63132"
            ]
          }
        },
        "3fa1c5bc88344682a1206a6071dbd444": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b2c10e212eab4810a4735048e0e75127": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_0f7ee74fba3246c1a7951810c13ddccf",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_babd67d53a294dffb8e42cf6b902da33"
          }
        },
        "ed838d06c88f473f8fc0abbf72f63132": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_6da0723c8c834627b0e9d16d8b45e5c1",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1999995/? [01:03&lt;00:00, 31584.64it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8e78c5b61eab47478b03cf5fa20845a4"
          }
        },
        "0f7ee74fba3246c1a7951810c13ddccf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "babd67d53a294dffb8e42cf6b902da33": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6da0723c8c834627b0e9d16d8b45e5c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8e78c5b61eab47478b03cf5fa20845a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lnRBD_ZFx5JU",
        "outputId": "44f51c2b-4ffc-49d5-aac9-f53e6dde1d88"
      },
      "source": [
        "import os\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download(\"all\")\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lcdpxwluRIjt"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm_notebook #notebook.tqdm\n",
        "from torch.utils.data import TensorDataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import RandomSampler\n",
        "from torch.utils.data import SequentialSampler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import random\n",
        "import time\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ix44NgcmyM-L",
        "outputId": "e7d05c18-3af5-45fe-ef87-de73fd081932"
      },
      "source": [
        "URL = 'https://www.cs.cornell.edu/people/pabo/movie-review-data/rt-polaritydata.tar.gz'\n",
        "# website from where we get the Movie Review (MR) Datasets from (Pang and Lee, 2005)\n",
        "!wget -P 'Data/' $URL\n",
        "# Unzipping the data \n",
        "!tar xvzf 'Data/rt-polaritydata.tar.gz' -C 'Data/'"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-05-20 17:20:32--  https://www.cs.cornell.edu/people/pabo/movie-review-data/rt-polaritydata.tar.gz\n",
            "Resolving www.cs.cornell.edu (www.cs.cornell.edu)... 132.236.207.36\n",
            "Connecting to www.cs.cornell.edu (www.cs.cornell.edu)|132.236.207.36|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 487770 (476K) [application/x-gzip]\n",
            "Saving to: â€˜Data/rt-polaritydata.tar.gzâ€™\n",
            "\n",
            "rt-polaritydata.tar 100%[===================>] 476.34K   509KB/s    in 0.9s    \n",
            "\n",
            "2021-05-20 17:20:34 (509 KB/s) - â€˜Data/rt-polaritydata.tar.gzâ€™ saved [487770/487770]\n",
            "\n",
            "rt-polaritydata.README.1.0.txt\n",
            "rt-polaritydata/rt-polarity.neg\n",
            "rt-polaritydata/rt-polarity.pos\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohPr2kn0yd41"
      },
      "source": [
        "def load_text(path):\n",
        "  file = open(path,'rb')\n",
        "  text = []\n",
        "  for line in file:\n",
        "    text.append(line.decode(errors='ignore').lower().strip())\n",
        "  return text\n",
        "\n",
        "neg_text = load_text('Data/rt-polaritydata/rt-polarity.neg')\n",
        "pos_text = load_text('Data/rt-polaritydata/rt-polarity.pos')\n",
        "\n",
        "texts = np.array(neg_text + pos_text)\n",
        "labels = np.array([0]*len(neg_text) + [1]*len(pos_text))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8UH_ZrN8kVB3",
        "outputId": "45d0abb9-8db4-4439-dfa9-3bfc51413172"
      },
      "source": [
        "print(len(texts))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10662\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i_6C-PHsyyGU",
        "outputId": "6f836eb8-88a4-411b-80eb-f9f99c027a5e"
      },
      "source": [
        "URL = \"https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M.vec.zip\"\n",
        "FILE = \"fastText\"\n",
        "\n",
        "if os.path.isdir(FILE):\n",
        "    print(\"fastText exists.\")\n",
        "else:\n",
        "    !wget -P $FILE $URL\n",
        "    !unzip $FILE/crawl-300d-2M.vec.zip -d $FILE"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-05-20 17:20:34--  https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M.vec.zip\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 172.67.9.4, 104.22.74.142, 104.22.75.142, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|172.67.9.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1523785255 (1.4G) [application/zip]\n",
            "Saving to: â€˜fastText/crawl-300d-2M.vec.zipâ€™\n",
            "\n",
            "crawl-300d-2M.vec.z 100%[===================>]   1.42G  12.0MB/s    in 2m 3s   \n",
            "\n",
            "2021-05-20 17:22:38 (11.8 MB/s) - â€˜fastText/crawl-300d-2M.vec.zipâ€™ saved [1523785255/1523785255]\n",
            "\n",
            "Archive:  fastText/crawl-300d-2M.vec.zip\n",
            "  inflating: fastText/crawl-300d-2M.vec  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_F1sQPfy1Gq",
        "outputId": "a76eb2a8-9ca1-42fc-c711-1feb057ecab6"
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"No. of GPU(s) available: {}\".format(torch.cuda.device_count()))\n",
        "    print('Device name:', torch.cuda.get_device_name(0))       \n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No. of GPU(s) available: 1\n",
            "Device name: Tesla K80\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FsUXZurBzqIH"
      },
      "source": [
        "def tokenize(texts):\n",
        "\n",
        "    max_len = 0\n",
        "    tokenized_texts = []\n",
        "    word2idx = {}\n",
        "    lengths_of_sentences = []\n",
        "\n",
        "    # <pad> refers to padding and <unk> refers to unknown word \n",
        "    word2idx['<pad>'] = 0\n",
        "    word2idx['<unk>'] = 1\n",
        "\n",
        "    idx = 2\n",
        "    for sent in texts:\n",
        "        tokenized_sent = word_tokenize(sent)\n",
        "        tokenized_texts.append(tokenized_sent)\n",
        "        count = 0\n",
        "        \n",
        "        for token in tokenized_sent:\n",
        "            count+=1\n",
        "            if token not in word2idx:\n",
        "                word2idx[token] = idx\n",
        "                idx += 1\n",
        "        lengths_of_sentences.append(count)\n",
        "                \n",
        "        max_len = max(max_len, len(tokenized_sent))\n",
        "    lengths_of_sentences = np.array(lengths_of_sentences)\n",
        "    print(\"Average Sentence Length = {}\".format(np.mean(lengths_of_sentences)))\n",
        "\n",
        "    return tokenized_texts, word2idx, max_len"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3dXKaceES6yd"
      },
      "source": [
        "def encode(tokenized_texts, word2idx, max_len):\n",
        "    input_ids = []\n",
        "    for tokenized_sent in tokenized_texts:\n",
        "        # Padding the sentences to max_len\n",
        "        tokenized_sent += ['<pad>'] * (max_len - len(tokenized_sent))\n",
        "\n",
        "        # Encode tokens to input_ids\n",
        "        input_id = [word2idx.get(token) for token in tokenized_sent]\n",
        "        input_ids.append(input_id)\n",
        "    \n",
        "    return np.array(input_ids)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Sivgfza0Rhk"
      },
      "source": [
        "def load_pretrained_vectors(word2idx, fname):\n",
        "    print(\"Loading pretrained vectors...\")\n",
        "    fin = open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
        "    n, d = map(int, fin.readline().split())\n",
        "\n",
        "    # Initilize random embeddings\n",
        "    embeddings = np.random.uniform(-0.25, 0.25, (len(word2idx), d))\n",
        "    embeddings[word2idx['<pad>']] = np.zeros((d,))\n",
        "\n",
        "    # Load pretrained vectors\n",
        "    count = 0\n",
        "    for line in tqdm_notebook(fin):\n",
        "        tokens = line.rstrip().split(' ')\n",
        "        word = tokens[0]\n",
        "        if word in word2idx:\n",
        "            count += 1\n",
        "            embeddings[word2idx[word]] = np.array(tokens[1:], dtype=np.float32)\n",
        "\n",
        "    print(\"There are {} / {} pretrained vectors found.\".format(count,len(word2idx)))\n",
        "\n",
        "    return embeddings\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202,
          "referenced_widgets": [
            "d5ac1e702a2c49c7aed8a86c4c1f904d",
            "3fa1c5bc88344682a1206a6071dbd444",
            "b2c10e212eab4810a4735048e0e75127",
            "ed838d06c88f473f8fc0abbf72f63132",
            "0f7ee74fba3246c1a7951810c13ddccf",
            "babd67d53a294dffb8e42cf6b902da33",
            "6da0723c8c834627b0e9d16d8b45e5c1",
            "8e78c5b61eab47478b03cf5fa20845a4"
          ]
        },
        "id": "SKrX4Ivt0YO-",
        "outputId": "3bb87232-c5f9-4c8c-a64b-758578a8ae16"
      },
      "source": [
        "# Tokenize, build vocabulary, encode tokens\n",
        "print(\"Tokenizing...\\n\")\n",
        "tokenized_texts, word2idx, max_len = tokenize(texts)\n",
        "input_ids = encode(tokenized_texts, word2idx, max_len)\n",
        "\n",
        "# Load pretrained vectors\n",
        "embeddings = load_pretrained_vectors(word2idx, \"fastText/crawl-300d-2M.vec\")\n",
        "embeddings = torch.tensor(embeddings)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokenizing...\n",
            "\n",
            "Average Sentence Length = 21.5834740198837\n",
            "Loading pretrained vectors...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:12: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  if sys.path[0] == '':\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d5ac1e702a2c49c7aed8a86c4c1f904d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "There are 18526 / 20286 pretrained vectors found.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67zgPtwhlRgd",
        "outputId": "72904379-8878-49c7-8eb2-e96b14cde2e3"
      },
      "source": [
        "print(max_len)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "62\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfgSSSG20a80"
      },
      "source": [
        "def data_loader(train_inputs, val_inputs, train_labels, val_labels,batch_size=50):\n",
        "\n",
        "    train_inputs = torch.tensor(train_inputs)\n",
        "    train_labels = torch.tensor(train_labels)\n",
        "    val_inputs = torch.tensor(val_inputs)\n",
        "    val_labels = torch.tensor(val_labels)\n",
        "\n",
        "    # Create DataLoader for training data\n",
        "    train_data = TensorDataset(train_inputs, train_labels)\n",
        "    train_sampler = RandomSampler(train_data)\n",
        "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=50)\n",
        "\n",
        "    # Create DataLoader for validation data\n",
        "    val_data = TensorDataset(val_inputs, val_labels)\n",
        "    val_sampler = SequentialSampler(val_data)\n",
        "    val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n",
        "\n",
        "    return train_dataloader, val_dataloader"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1_raeE30g1y"
      },
      "source": [
        "\n",
        "\n",
        "# Train Test Split\n",
        "train_inputs, val_inputs, train_labels, val_labels = train_test_split(input_ids, labels, test_size=0.1, random_state=42)\n",
        "\n",
        "# Load data to PyTorch DataLoader\n",
        "train_dataloader, val_dataloader = \\\n",
        "data_loader(train_inputs, val_inputs, train_labels, val_labels, batch_size=50)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57ws4BR706wm"
      },
      "source": [
        "class CNN_NLP(nn.Module):\n",
        "    def __init__(self,\n",
        "                 pretrained_embedding=None,\n",
        "                 freeze_embedding=False,\n",
        "                 vocab_size=None,\n",
        "                 embed_dim=300,\n",
        "                 filter_sizes=[3, 4, 5],\n",
        "                 num_filters=[100, 100, 100],\n",
        "                 num_classes=2,\n",
        "                 dropout=0.5):\n",
        "\n",
        "        super(CNN_NLP, self).__init__()\n",
        "        # Embedding layer\n",
        "        if pretrained_embedding is not None:\n",
        "            self.vocab_size, self.embed_dim = pretrained_embedding.shape\n",
        "            self.embedding = nn.Embedding.from_pretrained(pretrained_embedding,\n",
        "                                                          freeze=freeze_embedding)\n",
        "        else:\n",
        "            self.embed_dim = embed_dim\n",
        "            self.embedding = nn.Embedding(num_embeddings=vocab_size,\n",
        "                                          embedding_dim=self.embed_dim,\n",
        "                                          padding_idx=0,\n",
        "                                          max_norm=5.0)\n",
        "        # Conv Network\n",
        "        self.conv1d_list = nn.ModuleList([\n",
        "            nn.Conv1d(in_channels=self.embed_dim,\n",
        "                      out_channels=num_filters[i],\n",
        "                      kernel_size=filter_sizes[i])\n",
        "            for i in range(len(filter_sizes))\n",
        "        ])\n",
        "        # Fully-connected layer and Dropout\n",
        "        self.fc = nn.Linear(np.sum(num_filters), num_classes)\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "\n",
        "        x_embed = self.embedding(input_ids).float()\n",
        "        x_reshaped = x_embed.permute(0, 2, 1)\n",
        "        #Applying CNN and Relu\n",
        "        x_conv_list = [F.relu(conv1d(x_reshaped)) for conv1d in self.conv1d_list]\n",
        "\n",
        "        # Max pooling\n",
        "        x_pool_list = [F.max_pool1d(x_conv, kernel_size=x_conv.shape[2])\n",
        "            for x_conv in x_conv_list]\n",
        "        \n",
        "        x_fc = torch.cat([x_pool.squeeze(dim=2) for x_pool in x_pool_list],\n",
        "                         dim=1)\n",
        "        \n",
        "        logits = self.fc(self.dropout(x_fc))\n",
        "\n",
        "        return logits"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KEUtyZnP1H5X"
      },
      "source": [
        "def initilize_model(pretrained_embedding=None,\n",
        "                    freeze_embedding=False,\n",
        "                    vocab_size=None,\n",
        "                    embed_dim=300,\n",
        "                    filter_sizes=[3, 4, 5],\n",
        "                    num_filters=[100, 100, 100],\n",
        "                    num_classes=2,\n",
        "                    dropout=0.5,\n",
        "                    learning_rate=0.01):\n",
        "\n",
        "    assert (len(filter_sizes) == len(num_filters)), \"filter_sizes and \\\n",
        "    num_filters need to be of the same length.\"\n",
        "\n",
        "    # Instantiate CNN model\n",
        "    cnn_model = CNN_NLP(pretrained_embedding=pretrained_embedding,\n",
        "                        freeze_embedding=freeze_embedding,\n",
        "                        vocab_size=vocab_size,\n",
        "                        embed_dim=embed_dim,\n",
        "                        filter_sizes=filter_sizes,\n",
        "                        num_filters=num_filters,\n",
        "                        num_classes=2,\n",
        "                        dropout=0.5)\n",
        "    \n",
        "    cnn_model.to(device)\n",
        "\n",
        "    # Instantiate Adadelta optimizer\n",
        "    optimizer = optim.Adadelta(cnn_model.parameters(),\n",
        "                               lr=learning_rate,\n",
        "                               rho=0.95)\n",
        "\n",
        "    return cnn_model, optimizer\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlXLsNqO1QNr"
      },
      "source": [
        "# Specify loss function\n",
        "#Here we are using cross entropy loss function\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "def set_seed(seed_value=42):\n",
        "\n",
        "    random.seed(seed_value)\n",
        "    np.random.seed(seed_value)\n",
        "    torch.manual_seed(seed_value)\n",
        "    torch.cuda.manual_seed_all(seed_value)\n",
        "\n",
        "def train(model, optimizer, train_dataloader, val_dataloader=None, epochs=10):\n",
        "    \n",
        "    # Tracking best validation accuracy\n",
        "    best_accuracy = 0\n",
        "\n",
        "    # Start training loop\n",
        "    print(\"Start training...\\n\")\n",
        "    print(f\"{'Epoch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {\\\n",
        "    'Val Acc':^9} | {'Elapsed':^9}\")\n",
        "    print(\"-\"*60)\n",
        "\n",
        "    for epoch_i in range(epochs):\n",
        "        # =======================================\n",
        "        #               Training\n",
        "        # =======================================\n",
        "\n",
        "        # Tracking time and loss\n",
        "        t0_epoch = time.time()\n",
        "        total_loss = 0\n",
        "\n",
        "        # Put the model into the training mode\n",
        "        model.train()\n",
        "\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            # Load batch to GPU\n",
        "            b_input_ids, b_labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "            # Zero out any previously calculated gradients\n",
        "            model.zero_grad()\n",
        "\n",
        "            # Perform a forward pass. This will return logits.\n",
        "            logits = model(b_input_ids)\n",
        "\n",
        "            # Compute loss and accumulate the loss values\n",
        "            loss = loss_fn(logits, b_labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Perform a backward pass to calculate gradients\n",
        "            loss.backward()\n",
        "\n",
        "            # Update parameters\n",
        "            optimizer.step()\n",
        "\n",
        "        # Calculate the average loss over the entire training data\n",
        "        avg_train_loss = total_loss / len(train_dataloader)\n",
        "\n",
        "        # =======================================\n",
        "        #               Evaluation\n",
        "        # =======================================\n",
        "        if val_dataloader is not None:\n",
        "            # After the completion of each training epoch, measure the model's\n",
        "            # performance on our validation set.\n",
        "            val_loss, val_accuracy = evaluate(model, val_dataloader)\n",
        "\n",
        "            # Track the best accuracy\n",
        "            if val_accuracy > best_accuracy:\n",
        "                best_accuracy = val_accuracy\n",
        "\n",
        "            # Print performance over the entire training data\n",
        "            time_elapsed = time.time() - t0_epoch\n",
        "            print(f\"{epoch_i + 1:^7} | {avg_train_loss:^12.6f} | {\\\n",
        "            val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n",
        "            \n",
        "    print(\"\\n\")\n",
        "    print(f\"Training complete! Best accuracy: {best_accuracy:.2f}%.\")\n",
        "\n",
        "def evaluate(model, val_dataloader):\n",
        "    # Put the model into the evaluation mode. The dropout layers are disabled\n",
        "    # during the test time.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables\n",
        "    val_accuracy = []\n",
        "    val_loss = []\n",
        "\n",
        "    # For each batch in our validation set...\n",
        "    for batch in val_dataloader:\n",
        "        # Load batch to GPU\n",
        "        b_input_ids, b_labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "        # Compute logits\n",
        "        with torch.no_grad():\n",
        "            logits = model(b_input_ids)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = loss_fn(logits, b_labels)\n",
        "        val_loss.append(loss.item())\n",
        "\n",
        "        # Get the predictions\n",
        "        preds = torch.argmax(logits, dim=1).flatten()\n",
        "\n",
        "        # Calculate the accuracy rate\n",
        "        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
        "        val_accuracy.append(accuracy)\n",
        "\n",
        "    # Compute the average accuracy and loss over the validation set.\n",
        "    val_loss = np.mean(val_loss)\n",
        "    val_accuracy = np.mean(val_accuracy)\n",
        "\n",
        "    return val_loss, val_accuracy"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhmcDNYY1jUV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffc396f7-9850-458c-ead4-481881580198"
      },
      "source": [
        "# CNN-rand: Word vectors are randomly initialized.\n",
        "set_seed(42)\n",
        "cnn_rand, optimizer = initilize_model(vocab_size=len(word2idx),\n",
        "                                      embed_dim=300,\n",
        "                                      learning_rate=0.25,\n",
        "                                      dropout=0.5)\n",
        "train(cnn_rand, optimizer, train_dataloader, val_dataloader, epochs=20)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start training...\n",
            "\n",
            " Epoch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "------------------------------------------------------------\n",
            "   1    |   0.685046   |  0.656949  |   62.76   |   3.25   \n",
            "   2    |   0.623783   |  0.614303  |   66.75   |   2.84   \n",
            "   3    |   0.545940   |  0.585377  |   69.74   |   2.85   \n",
            "   4    |   0.458083   |  0.559708  |   71.11   |   2.84   \n",
            "   5    |   0.377395   |  0.533691  |   71.21   |   2.86   \n",
            "   6    |   0.298174   |  0.534345  |   71.48   |   2.85   \n",
            "   7    |   0.241923   |  0.541917  |   72.29   |   2.85   \n",
            "   8    |   0.183822   |  0.527530  |   72.66   |   2.84   \n",
            "   9    |   0.145478   |  0.539499  |   73.39   |   2.85   \n",
            "  10    |   0.121752   |  0.547218  |   72.57   |   2.85   \n",
            "  11    |   0.092083   |  0.574864  |   73.75   |   2.85   \n",
            "  12    |   0.072628   |  0.568181  |   73.11   |   2.85   \n",
            "  13    |   0.065201   |  0.590676  |   72.03   |   2.84   \n",
            "  14    |   0.049276   |  0.587760  |   73.03   |   2.84   \n",
            "  15    |   0.044585   |  0.599673  |   74.20   |   2.85   \n",
            "  16    |   0.039970   |  0.618002  |   73.74   |   2.85   \n",
            "  17    |   0.033013   |  0.631914  |   73.84   |   2.85   \n",
            "  18    |   0.030087   |  0.635638  |   72.66   |   2.85   \n",
            "  19    |   0.028315   |  0.655832  |   73.03   |   2.83   \n",
            "  20    |   0.023705   |  0.668683  |   73.03   |   2.84   \n",
            "\n",
            "\n",
            "Training complete! Best accuracy: 74.20%.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQdwg9Q81onZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4baf5c21-8881-45c2-9360-24e28dcc7a10"
      },
      "source": [
        "# CNN-static: fastText pretrained word vectors are used and freezed during training.\n",
        "set_seed(42)\n",
        "cnn_static, optimizer = initilize_model(pretrained_embedding=embeddings,\n",
        "                                        freeze_embedding=True,\n",
        "                                        learning_rate=0.25,\n",
        "                                        dropout=0.5)\n",
        "train(cnn_static, optimizer, train_dataloader, val_dataloader, epochs=20)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start training...\n",
            "\n",
            " Epoch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "------------------------------------------------------------\n",
            "   1    |   0.587964   |  0.476038  |   76.84   |   1.49   \n",
            "   2    |   0.454366   |  0.436961  |   79.75   |   1.49   \n",
            "   3    |   0.388918   |  0.421341  |   81.47   |   1.47   \n",
            "   4    |   0.338216   |  0.409772  |   81.11   |   1.46   \n",
            "   5    |   0.286697   |  0.410781  |   82.02   |   1.48   \n",
            "   6    |   0.238443   |  0.412215  |   82.02   |   1.47   \n",
            "   7    |   0.186918   |  0.411270  |   82.57   |   1.46   \n",
            "   8    |   0.149533   |  0.417292  |   81.84   |   1.47   \n",
            "   9    |   0.117995   |  0.424606  |   82.74   |   1.46   \n",
            "  10    |   0.093353   |  0.440678  |   82.84   |   1.47   \n",
            "  11    |   0.076145   |  0.453994  |   82.83   |   1.47   \n",
            "  12    |   0.059487   |  0.465853  |   82.48   |   1.48   \n",
            "  13    |   0.047119   |  0.486671  |   82.02   |   1.48   \n",
            "  14    |   0.039800   |  0.499319  |   81.93   |   1.46   \n",
            "  15    |   0.035278   |  0.512999  |   81.75   |   1.48   \n",
            "  16    |   0.032121   |  0.511619  |   82.84   |   1.47   \n",
            "  17    |   0.027908   |  0.532308  |   82.29   |   1.48   \n",
            "  18    |   0.022876   |  0.541484  |   81.57   |   1.47   \n",
            "  19    |   0.019924   |  0.551528  |   82.47   |   1.47   \n",
            "  20    |   0.015067   |  0.568064  |   81.21   |   1.47   \n",
            "\n",
            "\n",
            "Training complete! Best accuracy: 82.84%.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DhlLFCAV16GZ",
        "outputId": "112d00f6-1b94-4ca2-b6b0-89ece9e8cb4e"
      },
      "source": [
        "# CNN-non-static: fastText pretrained word vectors are fine-tuned during training.\n",
        "set_seed(42)\n",
        "cnn_non_static, optimizer = initilize_model(pretrained_embedding=embeddings,\n",
        "                                            freeze_embedding=False,\n",
        "                                            learning_rate=0.25,\n",
        "                                            dropout=0.5)\n",
        "train(cnn_non_static, optimizer, train_dataloader, val_dataloader, epochs=20)\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start training...\n",
            "\n",
            " Epoch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "------------------------------------------------------------\n",
            "   1    |   0.587139   |  0.474217  |   76.94   |   3.89   \n",
            "   2    |   0.450854   |  0.433970  |   79.75   |   3.89   \n",
            "   3    |   0.380891   |  0.416374  |   81.38   |   3.91   \n",
            "   4    |   0.324834   |  0.403107  |   81.39   |   3.91   \n",
            "   5    |   0.270032   |  0.401507  |   81.83   |   3.91   \n",
            "   6    |   0.219365   |  0.401453  |   82.48   |   3.94   \n",
            "   7    |   0.165141   |  0.407317  |   82.11   |   3.91   \n",
            "   8    |   0.127560   |  0.420063  |   82.84   |   3.91   \n",
            "   9    |   0.098052   |  0.436551  |   83.47   |   3.90   \n",
            "  10    |   0.077530   |  0.454695  |   82.84   |   3.90   \n",
            "  11    |   0.059240   |  0.484817  |   82.47   |   3.92   \n",
            "  12    |   0.045872   |  0.492505  |   82.83   |   3.91   \n",
            "  13    |   0.035400   |  0.517293  |   82.48   |   3.90   \n",
            "  14    |   0.029224   |  0.533558  |   82.29   |   3.90   \n",
            "  15    |   0.025155   |  0.538827  |   82.93   |   3.91   \n",
            "  16    |   0.021260   |  0.557047  |   82.83   |   3.90   \n",
            "  17    |   0.019693   |  0.584427  |   82.11   |   3.90   \n",
            "  18    |   0.016205   |  0.590658  |   82.74   |   3.91   \n",
            "  19    |   0.012503   |  0.595527  |   83.29   |   3.90   \n",
            "  20    |   0.010228   |  0.618876  |   82.47   |   3.90   \n",
            "\n",
            "\n",
            "Training complete! Best accuracy: 83.47%.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YD-SxLTi2ELo"
      },
      "source": [
        "def predict(text, model=cnn_non_static.to(\"cpu\"), max_len=62):\n",
        "    \"\"\"Predict probability that a review is positive.\"\"\"\n",
        "\n",
        "    # Tokenize, pad and encode text\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    padded_tokens = tokens + ['<pad>'] * (max_len - len(tokens))\n",
        "    input_id = [word2idx.get(token, word2idx['<unk>']) for token in padded_tokens]\n",
        "\n",
        "    # Convert to PyTorch tensors\n",
        "    input_id = torch.tensor(input_id).unsqueeze(dim=0)\n",
        "\n",
        "    # Compute logits\n",
        "    logits = model.forward(input_id)\n",
        "\n",
        "    #  Compute probability\n",
        "    probs = F.softmax(logits, dim=1).squeeze(dim=0)\n",
        "\n",
        "    print(f\"This review is {probs[1] * 100:.2f}% positive.\")"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xt_s6LLj2TaJ",
        "outputId": "7502f8fd-194b-4685-d88c-6022b8c6f049"
      },
      "source": [
        "predict(\"All of friends slept while watching this movie. But I really enjoyed it.\")\n",
        "predict(\"I have waited so long for this movie. I am now so satisfied and happy.\")\n",
        "predict(\"This movie is long and boring.\")\n",
        "predict(\"I don't like the ending.\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This review is 44.33% positive.\n",
            "This review is 97.01% positive.\n",
            "This review is 0.01% positive.\n",
            "This review is 8.94% positive.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HrI_PEQv2bET",
        "outputId": "c435df9d-cb27-4ef6-e5a8-cfeaf9be2c64"
      },
      "source": [
        "predict(\"This movie was not ok to watch \")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This review is 0.28% positive.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6WzYhCl37Ad"
      },
      "source": [
        ""
      ],
      "execution_count": 23,
      "outputs": []
    }
  ]
}